---
title: "MLHW6"
author: "Qinyuan Xing"
date: "2022/4/6"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = F, results = "markup", message = F)
options(scientific=T, digits = 3) 
```

## Q1
### (a)
```{r}
library(rpart)
library(dplyr)
library(ipred)
covid = data.frame(read.csv("E:\\2022 Spring\\R\\covid_data_pdb_v3.csv"))
covid2 = select(covid,c(pct_URBANIZED_AREA_POP_CEN_2010,pct_Males_ACS_14_18,pct_Pop_under_5_ACS_14_18,pct_Pop_5_17_ACS_14_18,pct_Pop_25_44_ACS_14_18,pct_Pop_45_64_ACS_14_18,pct_Pop_65plus_ACS_14_18,pct_Renter_Occp_HU_ACS_14_18,pct_Vacant_Units_ACS_14_18,pct_Mobile_Homes_ACS_14_18,pct_HHD_NoCompDevic_ACS_14_18,pct_HHD_No_Internet_ACS_14_18,pct_Hispanic_ACS_14_18,pct_NH_White_alone_ACS_14_18,pct_NH_Blk_alone_ACS_14_18,pct_Schl_Enroll_3_4_ACS_14_18,pct_Prs_Blw_Pov_Lev_ACS_14_18,high_community_level))
covid2$high_community_level = ifelse(covid2$high_community_level==FALSE,0,1)
covid2$high_community_level = as.factor(covid2$high_community_level)
# Bagging
ntree = c(10:50)
erroob = NULL
for (i in 1:41) {
  set.seed(123)
  mod = bagging(high_community_level~.,data = covid2,coob=T,nbagg=ntree[i])
  erroob[i] = mod$err
}
bagdata = data.frame(ntree=ntree,error=erroob)
bagdata
bagdata$ntree[which.min(bagdata$error)]
set.seed(123)
bagmod=adabag::bagging(high_community_level~.,data = covid2,mfinal = 47)
set.seed(123)
bagmod2 = ipred::bagging(high_community_level~.,data = covid2,coob=T,nbagg=47)

# Random Forests
library(randomForest)
rfgrid = expand.grid(mtry = c(1:17),ntree=seq(10,200,by=10),error=0)
for (i in 1:nrow(rfgrid)) {
  set.seed(123)
  mod2 = randomForest(high_community_level~.,data = covid2,ntree=rfgrid$ntree[i],mtry=rfgrid$mtry[i])
  rfgrid$error[i]=mean(mod2$err.rate[,1])
}
rfgrid[which.min(rfgrid$error),]
set.seed(123)
rfmod = randomForest(high_community_level~.,data = covid2,ntree=200,mtry=5)

# Adaboosting
library(gbm)
adagrid = expand.grid(ntree=seq(10,500,by=10),shrinkage = c(0.01,0.1,0.3),cverror=0)

for (i in 1:nrow(adagrid)) {
  set.seed(123)
  mod3 = gbm(high_community_level~.,data = covid2,distribution = "adaboost",cv.folds = 10,n.trees = adagrid$ntree[i],shrinkage =    adagrid$shrinkage[i])
  adagrid$cverror[i] = min(mod3$cv.error)
}
adagrid[which.min(adagrid$cverror),]

adamod = gbm(high_community_level~.,data = covid2,distribution = "adaboost",cv.folds = 10,n.trees = 500,shrinkage =0.3)

# Gradient boosting
gradgrid = expand.grid(ntree=seq(10,500,by=10),shrinkage = c(0.01,0.1,0.3),cverror=0)
for (i in 1:nrow(gradgrid)) {
  set.seed(123)
  mod4 = gbm(high_community_level~.,data = covid2,distribution = "gaussian",cv.folds = 10,n.trees = gradgrid$ntree[i],shrinkage =    gradgrid$shrinkage[i])
  gradgrid$cverror[i] = min(mod4$cv.error)
}
gradgrid[which.min(gradgrid$cverror),]
set.seed(123)
gradmod = gbm(high_community_level~.,data = covid2,distribution = "gaussian",cv.folds = 10,n.trees = 220,shrinkage = 0.1)
```

### (b)
```{r}
# Bagging
plot(bagdata,main="Bagging")

# Random forests
par(mfrow=c(2,1))
plot(rfgrid$error~rfgrid$ntree+rfgrid$mtry,main="Rf")

# Adaboosting
par(mfrow=c(2,1))
plot(adagrid$cverror~adagrid$ntree+adagrid$shrinkage,main="Adaboost")

# Gradient
par(mfrow=c(2,1))
plot(gradgrid$cverror~gradgrid$ntree+gradgrid$shrinkage,main="gradboost")
```

### (c)
```{r}
data.frame(method = c("Bagging","RF","Adaboost","Gradboost"),error=c(min(bagdata$error),min(rfgrid$error),min(adagrid$cverror),min(gradgrid$cverror)))
```
We can see that adaboosting has much better performance.

### (d)
```{r}
# bagging
bagmod$importance
# rf
importance(rfmod)
#Ada
summary(adamod,plotit = F,method = relative.influence)
#Grad
summary(gradmod,plotit = F,method = relative.influence)
```
The variables with high influence is also we choose in the HW5

### (e)

I would like to choose the gradboosting method for the test error is smaller than others except adaboosting (It seems overfitting)

## Q2
As for depth-one trees, value of d is 1. Each tree is generated by splitting the data on only one predictor and the final model is formed by adding the shrunken version of them repeatedly. Hence, in the final model:
$$\hat f(x)=\sum_{b-1}^B\lambda\hat f^b(x)$$
each additive term will depend on only one predictor leading to an additive model.



